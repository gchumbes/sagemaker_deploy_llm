# Llama-3.2-1B-Instruct - SageMaker Deployment

Este repositorio contiene el modelo Llama 3.2 1B Instruct listo para ser desplegado en AWS SageMaker. Incluye scripts de inferencia, configuraciÃ³n y despliegue.

## ðŸ“¦ Estructura del Proyecto

```
Llama-3.2-1B-Instruct/
â”œâ”€â”€ code/                     # Carpeta obligatoria para SageMaker, contiene lÃ³gica de inferencia
â”‚   â”œâ”€â”€ inference.py          # Script que carga el modelo y define cÃ³mo manejar las peticiones
â”‚   â””â”€â”€ requirements.txt      # LibrerÃ­as necesarias para el entorno de inferencia
â”œâ”€â”€ deploy.py                 # Script para desplegar el modelo en SageMaker (opcional, fuera del tar)
â”œâ”€â”€ config.json
â”œâ”€â”€ generation_config.json
â”œâ”€â”€ LICENSE.txt
â”œâ”€â”€ model.safetensors
â”œâ”€â”€ original/
â”‚   â”œâ”€â”€ consolidated.00.pth
â”‚   â”œâ”€â”€ params.json
â”‚   â””â”€â”€ tokenizer.model
â”œâ”€â”€ special_tokens_map.json
â”œâ”€â”€ tokenizer_config.json
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ USE_POLICY.md
```

## ðŸ§¾ Instrucciones de Uso

1. **AÃ±adir la carpeta `code/` al directorio del modelo**, junto a los archivos de pesos (`model.safetensors`, `config.json`, etc.).
2. Comprimir el contenido del modelo (incluyendo `code/`) en un archivo `.tar.gz`.
3. Subir este archivo a un bucket de Amazon S3:
   ```bash
   aws s3 cp modelV4_Base.tar.gz s3://your-bucket-name/
   ```
4. Ejecutar el script `deploy.py` (opcional) desde tu entorno local o un notebook para desplegar el modelo en SageMaker. Este script **no debe incluirse en el `.tar.gz`**.

## ðŸ“Œ Requisitos

- AWS CLI configurado
- Permisos adecuados en IAM (ej. rol SageMaker)
- SageMaker Studio o Jupyter con soporte para PyTorch y Hugging Face

## ðŸ“« Contacto

Para dudas o soporte tÃ©cnico, contactar al equipo de ML o al responsable del despliegue.

